# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright Â© 2025 Synaptics Incorporated.

import os
import argparse
import logging
from abc import ABC, abstractmethod
from collections.abc import Sequence
from pathlib import Path
from typing import Final

import numpy as np
import onnx
import onnx_graphsurgeon as gs
from torq.utils.logging import add_logging_args, configure_logging

from ...utils.onnx import is_same_dtype, upgrade_model

logger = logging.getLogger("FP32-Converter")

_ONNX_DTYPE_MAPPING: Final[dict[str, onnx.TensorProto.DataType]] = {
    "fp32": onnx.TensorProto.FLOAT,
    "bf16": onnx.TensorProto.BFLOAT16,
    "fp16": onnx.TensorProto.FLOAT16,
    "int64": onnx.TensorProto.INT64,
    "uint64": onnx.TensorProto.UINT64,
    "int32": onnx.TensorProto.INT32,
    "uint32": onnx.TensorProto.UINT32,
    "int16": onnx.TensorProto.INT16,
    "uint16": onnx.TensorProto.UINT16,
    "int8": onnx.TensorProto.INT8,
    "uint8": onnx.TensorProto.UINT8,
}


class OnnxDtypeConverterBase(ABC):

    def __init__(
        self,
        original_dtype: str,
        export_dtype: str,
        convert_io: bool = False,
        direct_cast: bool = True,
    ):
        if export_dtype not in self.allowed_dtypes():
            raise ValueError(
                f"Invalid export dtype '{export_dtype}', select from {sorted(self.allowed_dtypes())}"
            )
        self._original_dtype_str = original_dtype
        self._export_dtype_str = export_dtype
        self._convert_io = convert_io
        self._direct_cast = direct_cast

        self._original_onnx_dtype = self._validate_dtype(original_dtype)
        self._export_onnx_dtype = self._validate_dtype(export_dtype)
        self._convert_exceptions: dict[str, str] = {}

    @staticmethod
    def _validate_dtype(dtype: str) -> onnx.TensorProto.DataType:
        onnx_dtype = _ONNX_DTYPE_MAPPING.get(dtype)
        if onnx_dtype is None:
            raise ValueError(
                f"Invalid dtype '{dtype}', select from {list(_ONNX_DTYPE_MAPPING.keys())}"
            )
        return onnx_dtype

    @staticmethod
    def _collect_all_graphs(root: gs.Graph) -> list[gs.Graph]:
        graphs: list[gs.Graph] = []
        queue: list[gs.Graph] = [root]

        while queue:
            g = queue.pop()
            graphs.append(g)
            for node in g.nodes:
                for attr_val in node.attrs.values():
                    if isinstance(attr_val, gs.Graph):
                        queue.append(attr_val)
                    elif isinstance(attr_val, list):
                        for v in attr_val:
                            if isinstance(v, gs.Graph):
                                queue.append(v)

        return graphs

    def _is_original_dtype(self, dtype) -> bool:
        return is_same_dtype(dtype, self._original_onnx_dtype)

    @classmethod
    @abstractmethod
    def allowed_dtypes(cls) -> tuple[str, ...]: ...

    @abstractmethod
    def _convert_graph(
        self,
        graph: gs.Graph,
    ): ...

    def _handle_cast(self, cast_node: gs.Node, graph: gs.Graph):
        assert isinstance(cast_node, gs.Node) and cast_node.op == "Cast", \
            f"Expected Cast node in {self.__class__.__name__}._handle_cast()"
        cast_out: gs.Variable = cast_node.outputs[0]
        if cast_out.name in self._convert_exceptions:
            logger.debug("Skipping dtype conversion of explicitly marked cast output '%s'", cast_out.name)
            return
        if self._direct_cast:
            cast_out.dtype = self._export_onnx_dtype
            cast_node.attrs["to"] = self._export_onnx_dtype
            logger.debug("Update Cast op '%s' to directly cast to %s", cast_node.name, self._export_dtype_str)
        else:
            self._convert_exceptions[cast_out.name] = "Cast output and direct_cast=False"
            consumers: list[gs.Node] = list(cast_out.outputs)
            out_new: gs.Variable = graph.layer(
                name=cast_out.name + f"_cast_{self._original_dtype_str}_to_{self._export_dtype_str}",
                op="Cast",
                inputs=[cast_out],
                outputs=[gs.Variable(cast_out.name + f"_{self._export_dtype_str}", dtype=self._export_onnx_dtype, shape=cast_out.shape)],
                attrs={"to": self._export_onnx_dtype},
            )[0]
            for consumer in consumers:
                for i, inp in enumerate(consumer.inputs):
                    if inp is cast_out:
                        consumer.inputs[i] = out_new
                        logger.debug("Add %s -> %s Cast node to feed '%s'", self._original_dtype_str, self._export_dtype_str, consumer.name)

    def _convert_tensor(
        self,
        tensor: gs.Variable | gs.Constant,
        node: gs.Node,
        graph: gs.Graph,
        idx: int | str,
        is_attr: bool
    ):
        tensor_dtype = getattr(tensor, "export_dtype", None) or tensor.dtype
        if not self._is_original_dtype(tensor_dtype):
            logger.debug("Skipping non-%s tensor '%s'", self._original_dtype_str, tensor.name)
            return
        if tensor.name in self._convert_exceptions:
            logger.debug("Skipping dtype conversion of explicitly marked tensor '%s' (%s)",
                         tensor.name, self._convert_exceptions[tensor.name])
            return

        if isinstance(tensor, gs.Variable):
            # special handling not needed for runtime tensors
            tensor.dtype = self._export_onnx_dtype
            logger.debug("Set dtype of tensor '%s' to %s", tensor.name, self._export_dtype_str)
        elif isinstance(tensor, gs.Constant):
            new_const_name: str = tensor.name + f"_{self._export_dtype_str}"
            if not (new_const := graph.tensors().get(new_const_name)):
                if self._original_onnx_dtype == onnx.TensorProto.FLOAT and self._export_onnx_dtype == onnx.TensorProto.BFLOAT16:
                    new_const = gs.Constant(
                        new_const_name,
                        tensor.values,
                        export_dtype=self._export_onnx_dtype
                    )
                else:
                    try:
                        np_type = onnx.helper.tensor_dtype_to_np_dtype(self._export_onnx_dtype)
                    except (TypeError, ValueError, KeyError):
                        raise RuntimeError(f"Unsupported tensor datatype {self._export_dtype_str}")
                    new_const = gs.Constant(
                        new_const_name,
                        tensor.values.astype(np_type)
                    )
            logger.debug("Add %s initializer '%s'", self._export_dtype_str, new_const.name)
            try:
                if is_attr:
                    assert isinstance(idx, str), "Node attribute index must be a string"
                    node.attrs[idx] = new_const
                    logger.debug("Set attr '%s' of node '%s' to '%s'", idx, node.name, new_const.name)
                else:
                    assert isinstance(idx, int), "Node input index must be an integer"
                    node.inputs[idx] = new_const
                    logger.debug("Set input %d of node '%s' to '%s'", idx, node.name, new_const.name)
            except (IndexError, ValueError, KeyError):
                typ = "attribute" if is_attr else "input"
                logger.exception("Failed to update %s %s ('%s') of node '%s'", typ, idx, tensor.name, node.name)
        else:
            logger.warning("Skipping conversion due to invalid tensor type '%s'", str(type(tensor)))

    def _convert_node_io(
        self,
        node: gs.Node,
        graph: gs.Graph,
        skip_names: set[str]
    ):
        for i, inp in enumerate(list(node.inputs)):
            if inp.name in skip_names:
                logger.debug("Skipping dtype conversion of graph input '%s'", inp.name)
                continue
            self._convert_tensor(inp, node, graph, i, is_attr=False)

        for out in node.outputs:
            assert isinstance(out, gs.Variable), f"Non gs.Variable output '{out.name}' ({type(out)}) for node '{node.name}'"
            if out.name in skip_names:
                logger.debug("Skipping dtype conversion of graph output '%s'", out.name)
                continue
            if self._is_original_dtype(out.dtype):
                out.dtype = self._export_onnx_dtype
                if node.op == "Cast":
                    node.attrs["to"] = self._export_onnx_dtype

    def _update_input(self, graph: gs.Graph, inp: gs.Variable):
        if self._convert_io:
            inp.dtype = self._export_onnx_dtype
            logger.debug("Set dtype to %s for input '%s'", self._export_dtype_str, inp.name)
        else:
            # add cast nodes at model inputs
            consumers = list(inp.outputs)
            if not consumers:
                return
            inp_new = graph.layer(
                name=inp.name + f"_to_{self._export_dtype_str}",
                op="Cast",
                inputs=[inp],
                outputs=[gs.Variable(inp.name + f"_{self._export_dtype_str}", dtype=self._export_onnx_dtype, shape=inp.shape)],
                attrs={"to": self._export_onnx_dtype}
            )[0]
            for node in consumers:
                for i, val in enumerate(node.inputs):
                    if val is inp:
                        node.inputs[i] = inp_new
                        logger.debug("Update node '%s' to accept %s input '%s'", node.name, self._export_dtype_str, inp_new.name)
            self._convert_exceptions[inp.name] = "graph input and convert_io=False"

    def _update_output(self, graph: gs.Graph, out: gs.Variable, out_idx: int):
        out.dtype = self._export_onnx_dtype
        logger.debug("Set dtype to %s for output '%s'", self._export_dtype_str, out.name)
        if not self._convert_io:
            # add cast nodes at model outputs
            out_name = out.name
            out.name = out.name + f"_{self._export_dtype_str}"
            out_new = graph.layer(
                name=out.name + f"_to_{self._original_dtype_str}",
                op="Cast",
                inputs=[out],
                outputs=[gs.Variable(out_name, dtype=self._original_onnx_dtype, shape=out.shape)],
                attrs={"to": self._original_onnx_dtype}
            )[0]
            graph.outputs[out_idx] = out_new
            logger.debug("Add %s cast node for output '%s'", self._original_dtype_str, out_name)
            self._convert_exceptions[out_name] = "graph output and convert_io=False"

    def _update_inputs(self, graph: gs.Graph):
        for graph_inp in list(graph.inputs):
            if not self._is_original_dtype(graph_inp.dtype):
                logger.debug("Skipping non-%s input '%s'", self._original_dtype_str, graph_inp.name)
                continue
            self._update_input(graph, graph_inp)

    def _update_outputs(self, graph: gs.Graph):
        for i, graph_out in enumerate(list(graph.outputs)):
            if not self._is_original_dtype(graph_out.dtype):
                logger.debug("Skipping non-%s output '%s'", self._original_dtype_str, graph_out.name)
                continue
            self._update_output(graph, graph_out, i)

    def _fold_literal_constants(self, graph: gs.Graph):
        for node in graph.nodes:
            if node.op != "Constant" or node.inputs:
                continue
            if not isinstance((value := node.attrs.get("value")), gs.Constant):
                continue
            const_node_out: gs.Variable = node.outputs[0]
            const_init: gs.Constant = gs.Constant(
                name=(const_node_out.name or node.name) + "_folded",
                values=value.values
            )
            consumers: list[gs.Node] = list(const_node_out.outputs)
            for consumer in consumers:
                for i, inp in enumerate(consumer.inputs):
                    if inp is const_node_out:
                        consumer.inputs[i] = const_init
            node.outputs.clear()
            logger.debug(
                "Graph '%s': folded literal Constant producer node for '%s' into graph initializer",
                graph.name, const_node_out.name
            )
        graph = graph.cleanup(
            remove_unused_graph_inputs=True,
            remove_unused_node_outputs=True
        ).toposort()

    def _check_tensor_dtypes(self, graph: gs.Graph) -> tuple[list[str], list[str]]:
        all_tensors: list[str] = []
        not_converted: list[str] = []
        for tensor_name, tensor in graph.tensors().items():
            tensor_dtype = getattr(tensor, "export_dtype", None) or tensor.dtype
            if self._is_original_dtype(tensor_dtype):
                if (exc_reason := self._convert_exceptions.get(tensor_name)) is None:
                    logger.warning(
                        "Graph '%s': tensor '%s' not converted to %s (unhandled)",
                        graph.name, tensor_name, self._export_dtype_str
                    )
                else:
                    logger.debug(
                        "Graph '%s': tensor '%s' not converted to %s (%s)",
                        graph.name, tensor_name, self._export_dtype_str, exc_reason
                    )
                not_converted.append(tensor_name)
            all_tensors.append(tensor_name)
        return all_tensors, not_converted

    def _check_conversion(self, graph: gs.Graph) -> tuple[list[str], list[str]]:
        graph.cleanup(
            remove_unused_graph_inputs=True,
            remove_unused_node_outputs=True
        ).toposort()
        return self._check_tensor_dtypes(graph)

    def convert_model(
        self,
        input_model: onnx.ModelProto
    ) -> onnx.ModelProto:
        try:
            input_model = onnx.shape_inference.infer_shapes(
                input_model,
                check_type=True,
                strict_mode=True,
            )
        except onnx.shape_inference.InferenceError as e:
            logger.warning(
                "ONNX shape inference failed; proceeding with original model: %s",
                e,
                exc_info=True,
            )
        try:
            onnx.checker.check_model(input_model, full_check=True)
        except onnx.checker.ValidationError as e:
            logger.warning(
                "ONNX model validation failed; model may be malformed: %s",
                e,
                exc_info=True,
            )

        root, *subgraphs = self._collect_all_graphs(gs.import_onnx(input_model))
        all_tensors: list[str] = []
        not_converted: list[str] = []

        # convert subgraphs (if any)
        for g in subgraphs:
            self._fold_literal_constants(g)
            self._convert_graph(g)
            self._update_inputs(g)
            self._update_outputs(g)
            conv_info = self._check_conversion(g)
            all_tensors += conv_info[0]
            not_converted += conv_info[1]

        # convert root graph
        self._fold_literal_constants(root)
        self._convert_graph(root)
        self._update_inputs(root)
        self._update_outputs(root)
        conv_info = self._check_conversion(root)
        all_tensors += conv_info[0]
        not_converted += conv_info[1]

        new_model = onnx.shape_inference.infer_shapes(
            gs.export_onnx(root), check_type=True, strict_mode=True, data_prop=len(subgraphs) == 0
        )
        new_model.ir_version = input_model.ir_version
        onnx.checker.check_model(new_model, full_check=True)

        all_tensors = set(all_tensors)
        not_converted = set(not_converted)
        total = len(all_tensors)
        failed = len(not_converted)
        converted = total - failed

        if total == 0:
            logger.info(
                "Conversion results: graph has no %s tensors to convert to %s",
                self._original_dtype_str, self._export_dtype_str
            )
        else:
            pct = (converted / total) * 100.0
            logger.info(
                "Conversion results: converted %d/%d (%.2f%%) %s tensors to %s",
                converted, total, pct, self._original_dtype_str, self._export_dtype_str
            )

        return new_model


class FP32Converter(OnnxDtypeConverterBase):

    def __init__(
        self,
        export_dtype: str,
        convert_io: bool = False,
        direct_cast: bool = True
    ):
        super().__init__("fp32", export_dtype, convert_io, direct_cast)

    @classmethod
    def allowed_dtypes(cls) -> tuple[str, ...]:
        return ("bf16", "fp16")

    def _convert_graph(
        self,
        graph: gs.Graph
    ):
        skip_names = {i.name for i in graph.inputs} | {o.name for o in graph.outputs}
        for node in list(graph.nodes):
            # special case: Cast -> properly handle casts to fp32
            if node.op == "Cast" and self._is_original_dtype(node.outputs[0].dtype):
                self._handle_cast(node, graph)
                continue

            # special case: DQL -> input and scale output must be fp32
            if node.op == "DynamicQuantizeLinear":
                inp_f32: gs.Variable = graph.layer(
                    name=node.name + "_inp_cast_f32",
                    op="Cast",
                    inputs=[node.inputs[0]],
                    outputs=[gs.Variable(node.name + "_inp_f32", dtype=np.float32, shape=node.inputs[0].shape)],
                    attrs={"to": onnx.TensorProto.FLOAT},
                )[0]
                node.inputs[0] = inp_f32
                self._convert_exceptions[inp_f32.name] = "DynamicQuantizeLinear input"
                logger.debug("Add %s -> fp32 Cast node to input of DQL node '%s'", self._export_dtype_str, node.name)

                scale_out: gs.Variable = node.outputs[1]
                self._convert_exceptions[scale_out.name] = "DynamicQuantizeLinear output"
                consumers: list[gs.Node] = list(scale_out.outputs)
                scale_out_f16: gs.Variable = graph.layer(
                    name=node.name + f"_scale_cast_{self._export_dtype_str}",
                    op="Cast",
                    inputs=[scale_out],
                    outputs=[gs.Variable(node.name + f"_scale_{self._export_dtype_str}", dtype=self._export_onnx_dtype, shape=scale_out.shape)],
                    attrs={"to": self._export_onnx_dtype},
                )[0]
                for consumer in consumers:
                    for i, inp in enumerate(consumer.inputs):
                        if inp is scale_out:
                            consumer.inputs[i] = scale_out_f16
                            logger.debug("Add fp32 -> %s Cast node to feed '%s'", self._export_dtype_str, consumer.name)
                continue

            # special case: Constant -> constant value stored as an attribute
            if node.op == "Constant" and (val := node.attrs.get("value")) is not None:
                self._convert_tensor(val, node, graph, "value", is_attr=True)

            # special case: ConstantOfShape -> constant value stored as an attribute
            if node.op == "ConstantOfShape" and (val := node.attrs.get("value")) is not None:
                self._convert_tensor(val, node, graph, "value", is_attr=True)

            # special case: Resize -> only input and output can be cast to bf16
            if node.op == "Resize" and node.inputs[0].name not in skip_names:
                self._convert_tensor(node.inputs[0], node, graph, 0, is_attr=False)
                if self._is_original_dtype((out := node.outputs[0]).dtype) and out.name not in skip_names:
                    out.dtype = self._export_onnx_dtype
                continue

            self._convert_node_io(node, graph, skip_names)

        logger.info("Updated graph '%s' dtypes to %s", graph.name, self._export_dtype_str)


class Int64Converter(OnnxDtypeConverterBase):

    # as of onnx v1.21.0
    _ENFORCED_INT64_IO: Final[dict[str, tuple[tuple[int, ...], ...]]] = {
        "ConstantOfShape": ((0,),), # v25: inputs: (input, )
        "Expand": ((1,),),          # v13: inputs: (shape, )
        "Pad": ((1, 3),),           # v25: inputs: (pads, axes)
        "ReduceMean": ((1,),),      # v18: inputs: (axes, )
        "Reshape": ((1,),),         # v25: inputs: (shape, )
        "Resize": ((3,),),          # v19: inputs: (sizes, )
        "Slice": ((1, 2, 3, 4),),   # v13: inputs: (starts, ends, axes, steps)
        "Squeeze": ((1,),),         # v25: inputs: (axes, )
        "Tile": ((1,),),            # v13: inputs: (repeats, )
        "TopK": ((1,), (1,)),       # v24: inputs: (K, ), outputs: (I, )
        "Unsqueeze": ((1,),),       # v25: inputs: (axes, )
    }

    def __init__(
        self,
        export_dtype: str = "int32",
        convert_io: bool = False,
        direct_cast: bool = True
    ):
        super().__init__("int64", export_dtype, convert_io, direct_cast)

        self._original_sdtype_str  = self._original_dtype_str
        self._original_udtype_str  = "u" + self._original_dtype_str
        self._export_sdtype_str    = self._export_dtype_str
        self._export_udtype_str    = "u" + self._export_dtype_str

        self._original_onnx_sdtype = self._original_onnx_dtype
        self._original_onnx_udtype = self._validate_dtype(self._original_udtype_str)
        self._export_onnx_sdtype   = self._export_onnx_dtype
        self._export_onnx_udtype   = self._validate_dtype(self._export_udtype_str)

    @classmethod
    def allowed_dtypes(cls) -> tuple[str, ...]:
        return ("int32", "int16", "int8")

    def _is_original_dtype(self, dtype) -> bool:
        return (
            is_same_dtype(dtype, self._original_onnx_sdtype)
            or is_same_dtype(dtype, self._original_onnx_udtype)
        )

    def _is_unsigned(self, dtype) -> bool:
        return is_same_dtype(dtype, self._original_onnx_udtype)

    def _set_dtypes(self, unsigned: bool):
        if unsigned:
            self._original_dtype_str  = self._original_udtype_str
            self._export_dtype_str    = self._export_udtype_str
            self._original_onnx_dtype = self._original_onnx_udtype
            self._export_onnx_dtype   = self._export_onnx_udtype
        else:
            self._original_dtype_str  = self._original_sdtype_str
            self._export_dtype_str    = self._export_sdtype_str
            self._original_onnx_dtype = self._original_onnx_sdtype
            self._export_onnx_dtype   = self._export_onnx_sdtype

    def _handle_cast(self, cast_node: gs.Node, graph: gs.Graph):
        self._set_dtypes(self._is_unsigned(cast_node.outputs[0].dtype))
        return super()._handle_cast(cast_node, graph)

    def _convert_tensor(
        self,
        tensor: gs.Variable | gs.Constant,
        node: gs.Node,
        graph: gs.Graph,
        idx: int | str,
        is_attr: bool
    ):
        dtype = getattr(tensor, "export_dtype", None) or tensor.dtype
        self._set_dtypes(self._is_unsigned(dtype))
        return super()._convert_tensor(tensor, node, graph, idx, is_attr)

    def _convert_node_io(
        self,
        node: gs.Node,
        graph: gs.Graph,
        skip_names: set[str]
    ):
        for i, inp in enumerate(list(node.inputs)):
            if inp.name in skip_names:
                logger.debug("Skipping dtype conversion of model input '%s'", inp.name)
                continue
            self._convert_tensor(inp, node, graph, i, is_attr=False)

        for out in node.outputs:
            assert isinstance(out, gs.Variable), f"Non gs.Variable output '{out.name}' ({type(out)}) for node '{node.name}'"
            if out.name in skip_names:
                logger.debug("Skipping dtype conversion of model output '%s'", out.name)
                continue
            if self._is_original_dtype(out.dtype):
                self._set_dtypes(self._is_unsigned(out.dtype))
                out.dtype = self._export_onnx_dtype
                if node.op == "Cast":
                    node.attrs["to"] = self._export_onnx_dtype

    def _convert_graph(
        self,
        graph: gs.Graph
    ):

        def _mark_no_convert(coll: Sequence[gs.Variable | gs.Constant], idx: int):
            if idx >= len(coll):
                return
            tensor = coll[idx]
            tensor_dtype = getattr(tensor, "export_dtype", None) or tensor.dtype
            if not self._is_original_dtype(tensor_dtype):
                # warn of ONNX spec mismatch
                dtype_str = onnx.helper.tensor_dtype_to_string(tensor_dtype) \
                    if isinstance (tensor_dtype, int) else str(tensor_dtype)
                logger.warning(
                    "Promoting to int64: tensor '%s' (dtype: %s), required for input %d of %s op '%s'",
                    tensor.name, dtype_str, idx, node.op, node.name,
                )
                tensor.dtype = onnx.TensorProto.INT64
            self._convert_exceptions[tensor.name] = f"{node.op} input {idx} requires int64"

        skip_names = {i.name for i in graph.inputs} | {o.name for o in graph.outputs}
        for node in list(graph.nodes):
            if node.op == "Cast" and self._is_original_dtype(node.outputs[0].dtype):
                self._handle_cast(node, graph)
                continue

            if node.op == "Constant" and not node.name and not node.inputs:
                if self._is_original_dtype(node.outputs[0].dtype):
                    logger.warning(
                        "Skipping int64 tensor %s originating from unnamed Constant node",
                        node.outputs[0].name
                    )
                    self._convert_exceptions[node.outputs[0].name] = \
                        "int64 output from unnamed Constant node"
                    continue

            if enforced := self._ENFORCED_INT64_IO.get(node.op):
                assert len(enforced) >= 1, "Invalid tensor indices for INT64 enforced I/O"
                inp_indices = enforced[0]
                out_indices = enforced[1] if len(enforced) > 1 else ()
                for idx in inp_indices:
                    _mark_no_convert(node.inputs, idx)
                for idx in out_indices:
                    _mark_no_convert(node.outputs, idx)

            self._convert_node_io(node, graph, skip_names)

        logger.info("Updated graph '%s' integer dtypes to %s", graph.name, self._export_dtype_str)

    def _update_input(self, graph: gs.Graph, inp: gs.Variable):
        self._set_dtypes(self._is_unsigned(inp.dtype))
        return super()._update_input(graph, inp)

    def _update_output(self, graph: gs.Graph, out: gs.Variable, out_idx: int):
        self._set_dtypes(self._is_unsigned(out.dtype))
        return super()._update_output(graph, out, out_idx)

    def _check_tensor_dtypes(self, graph: gs.Graph) -> tuple[list[str], list[str]]:
        self._export_dtype_str = self._export_sdtype_str
        return super()._check_tensor_dtypes(graph)


def _convert_modelopt(
    input_model: str | os.PathLike,
    convert_dtype: str,
    convert_io: bool,
    max_float: float,
):
    allowed_modelopt_dtypes = {"fp16", "bf16"}
    if convert_dtype not in allowed_modelopt_dtypes:
        raise ValueError(
            f"Invalid convert dtype '{convert_dtype}' for modelopt, "
            f"choose from {sorted(allowed_modelopt_dtypes)}"
        )
    try:
        from modelopt.onnx import autocast
    except ImportError as exc:
        raise RuntimeError(
            "Requested use_modelopt=True but TensorRT Model Optimizer "
            "('modelopt') is not installed. Install it or set use_modelopt=False."
        ) from exc

    return autocast.convert_to_mixed_precision(
        str(input_model),
        convert_dtype,
        data_max=max_float,
        init_max=max_float,
        keep_io_types=not convert_io,
    )


def _convert_internal(
    input_model: str | os.PathLike,
    convert_dtype: str,
    convert_io: bool,
    target_opset: int,
):
    model = onnx.load(input_model)
    model = upgrade_model(model, target_opset)

    if convert_dtype in FP32Converter.allowed_dtypes():
        converter = FP32Converter(convert_dtype, convert_io=convert_io)
    elif convert_dtype in Int64Converter.allowed_dtypes():
        converter = Int64Converter(convert_dtype, convert_io=convert_io)
    else:
        allowed_dtypes = list(FP32Converter.allowed_dtypes()) + list(Int64Converter.allowed_dtypes())
        raise ValueError(
            f"Invalid convert dtype '{convert_dtype}', choose from {allowed_dtypes}"
        )

    return converter.convert_model(model)


def convert_model(
    input_model: str | os.PathLike,
    output_model: str | os.PathLike,
    convert_dtype: str,
    use_modelopt: bool = False,
    convert_io: bool = False,
    max_float: float = 1e9,
    target_opset: int = 22,
):
    if use_modelopt:
        converted_model = _convert_modelopt(
            input_model,
            convert_dtype=convert_dtype,
            convert_io=convert_io,
            max_float=max_float,
        )
    else:
        converted_model = _convert_internal(
            input_model,
            convert_dtype=convert_dtype,
            convert_io=convert_io,
            target_opset=target_opset,
        )

    export_dir = Path(output_model).parent
    export_dir.mkdir(parents=True, exist_ok=True)
    onnx.save(converted_model, output_model)
    logger.info("Saved converted model to '%s'", str(output_model))


def add_onnx_dtype_convert_args(parser: argparse.ArgumentParser):
    parser.add_argument(
        "-i", "--input",
        type=str,
        required=True,
        help="Input ONNX model path"
    )
    parser.add_argument(
        "-o", "--output",
        type=str,
        required=True,
        help="Output ONNX model path"
    )
    parser.add_argument(
        "-d", "--dtype",
        type=str,
        metavar="DTYPE",
        choices=FP32Converter.allowed_dtypes() + Int64Converter.allowed_dtypes(),
        required=True,
        help="Export data type (choices: %(choices)s)"
    )
    parser.add_argument(
        "--opset",
        type=int,
        default=22,
        help="ONNX opset to use; note that a relatively new opset is required for bf16 support in some ops (default: %(default)s)"
    )
    parser.add_argument(
        "--max-float",
        type=float,
        default=1e9,
        help="Maximum FP32 value in model initializers and data, tensors with data > `max_float` will be left in fp32"
    )
    parser.add_argument(
        "--convert-io",
        action="store_true",
        default=False,
        help="Convert model I/O to export dtype"
    )
    parser.add_argument(
        "--modelopt",
        action="store_true",
        default=False,
        help="Use TensorRT modelopt for dtype conversion"
    )
    add_logging_args(parser)


def onnx_dtype_convert_from_args(args: argparse.Namespace):
    configure_logging(args.logging)
    convert_model(
        args.input,
        args.output,
        args.dtype,
        args.modelopt,
        args.convert_io,
        args.max_float,
        args.opset
    )


def main():
    parser = argparse.ArgumentParser()
    add_onnx_dtype_convert_args(parser)
    onnx_dtype_convert_from_args(parser.parse_args())


if __name__ == "__main__":
    main()
